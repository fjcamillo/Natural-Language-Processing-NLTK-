{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Notebook\n",
    "\n",
    "Tokenizer\n",
    "     - Word Tokenizer\n",
    "     - Sentence Tokenizer\n",
    "\n",
    "Corpora\n",
    "    - body of text, e.g. english language\n",
    "lexicon\n",
    "    - word and their meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Dear Mr. Francisc, Today we will be going to study about NLTK, a natural language processing library. Built for python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'Mr.', 'Francisc', ',', 'Today', 'we', 'will', 'be', 'going', 'to', 'study', 'about', 'NLTK', ',', 'a', 'natural', 'language', 'processing', 'library', '.', 'Built', 'for', 'python']\n"
     ]
    }
   ],
   "source": [
    "#prints out the tokenized text by word\n",
    "textReturn = word_tokenize(text)\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear Mr. Francisc, Today we will be going to study about NLTK, a natural language processing library.', 'Built for python']\n"
     ]
    }
   ],
   "source": [
    "words = sent_tokenize(text)\n",
    "\n",
    "#prints out the tokenized text by sentence\n",
    "print(sent_tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    " - stop words are words that will not give out information and may be neglected and not added to the feature set of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_words = [w for w in textReturn if not w in stop_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'Mr.', 'Francisc', ',', 'Today', 'going', 'study', 'NLTK', ',', 'natural', 'language', 'processing', 'library', '.', 'Built', 'python']\n"
     ]
    }
   ],
   "source": [
    "#printing filtered words\n",
    "print filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "- stemming is finding the word closest to the meaning of the actual word being used in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "pythonista\n",
      "pythonli\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "new_word = \"Good Morning, we are hear to learn about stemming, today we must know what is stemming and how it works\"\n",
    "related = ['python', 'pythoner', 'pythonista', 'pythonly', 'pythoning']\n",
    "\n",
    "for pyn in related:\n",
    "    print(ps.stem(pyn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good\n",
      "Morn\n",
      ",\n",
      "we\n",
      "are\n",
      "hear\n",
      "to\n",
      "learn\n",
      "about\n",
      "stem\n",
      ",\n",
      "today\n",
      "we\n",
      "must\n",
      "know\n",
      "what\n",
      "is\n",
      "stem\n",
      "and\n",
      "how\n",
      "it\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_word)\n",
    "for pyn in words:\n",
    "    print(ps.stem(pyn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "- part of speech tagging is tagging the different words in a sentence or paragraph on what they are.\n",
    "\n",
    "POS tag list:\n",
    "\n",
    "- CC    ==> coordinating conjunction\n",
    "- CD\t==> cardinal digit\n",
    "- DT\t==> determiner\n",
    "- EX    ==> existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW\t==> foreign word\n",
    "- IN\t==> preposition/subordinating conjunction\n",
    "- JJ\t==> adjective\t'big'\n",
    "- JJR\t==> adjective, comparative\t'bigger'\n",
    "- JJS\t==> adjective, superlative\t'biggest'\n",
    "- LS\t==> list marker\t1)\n",
    "- MD\t==> modal\tcould, will\n",
    "- NN\t==> noun, singular 'desk'\n",
    "- NNS\t==> noun plural\t'desks'\n",
    "- NNP\t==> proper noun, singular\t'Harrison'\n",
    "- NNPS\t==> proper noun, plural\t'Americans'\n",
    "- PDT\t==> predeterminer\t'all the kids'\n",
    "- POS\t==> possessive ending\tparent's\n",
    "- PRP\t==> personal pronoun\tI, he, she\n",
    "- PRP(DollarSign)\t==> possessive pronoun\tmy, his, hers\n",
    "- RB\t==> adverb\tvery, silently,\n",
    "- RBR\t==> adverb, comparative\tbetter\n",
    "- RBS\t==> adverb, superlative\tbest\n",
    "- RP\t==> particle\tgive up\n",
    "- TO\t==> to\tgo 'to' the store.\n",
    "- UH\t==> interjection\terrrrrrrrm\n",
    "- VB\t==> verb, base form\ttake\n",
    "- VBD\t==> verb, past tense\ttook\n",
    "- VBG\t==> verb, gerund/present participle\ttaking\n",
    "- VBN\t==> verb, past participle\ttaken\n",
    "- VBP\t==> verb, sing. present, non-3d\ttake\n",
    "- VBZ\t==> verb, 3rd person sing. present\ttakes\n",
    "- WDT\t==> wh-determiner\twhich\n",
    "- WP\t==> wh-pronoun\twho, what\n",
    "- WP(DollarSign)\t==> possessive wh-pronoun\twhose\n",
    "- WRB\t==> wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free contents from nltk, the following are from the state of the union address of George W. Bush\n",
    "train_set = state_union.raw('2005-GWBush.txt')\n",
    "test_set = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the training set, splitting them according to the punktsentencetokenizer algorithm\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing and testing with the test set\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'PRESIDENT', 'NNP'), (u'GEORGE', 'NNP'), (u'W.', 'NNP'), (u'BUSH', 'NNP'), (u\"'S\", 'POS'), (u'ADDRESS', 'NNP'), (u'BEFORE', 'IN'), (u'A', 'NNP'), (u'JOINT', 'NNP'), (u'SESSION', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'CONGRESS', 'NNP'), (u'ON', 'NNP'), (u'THE', 'NNP'), (u'STATE', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'UNION', 'NNP'), (u'January', 'NNP'), (u'31', 'CD'), (u',', ','), (u'2006', 'CD'), (u'THE', 'NNP'), (u'PRESIDENT', 'NNP'), (u':', ':'), (u'Thank', 'NNP'), (u'you', 'PRP'), (u'all', 'DT'), (u'.', '.')]\n",
      "[(u'Mr.', 'NNP'), (u'Speaker', 'NNP'), (u',', ','), (u'Vice', 'NNP'), (u'President', 'NNP'), (u'Cheney', 'NNP'), (u',', ','), (u'members', 'NNS'), (u'of', 'IN'), (u'Congress', 'NNP'), (u',', ','), (u'members', 'NNS'), (u'of', 'IN'), (u'the', 'DT'), (u'Supreme', 'NNP'), (u'Court', 'NNP'), (u'and', 'CC'), (u'diplomatic', 'JJ'), (u'corps', 'NN'), (u',', ','), (u'distinguished', 'JJ'), (u'guests', 'NNS'), (u',', ','), (u'and', 'CC'), (u'fellow', 'JJ'), (u'citizens', 'NNS'), (u':', ':'), (u'Today', 'VB'), (u'our', 'PRP$'), (u'nation', 'NN'), (u'lost', 'VBD'), (u'a', 'DT'), (u'beloved', 'VBN'), (u',', ','), (u'graceful', 'JJ'), (u',', ','), (u'courageous', 'JJ'), (u'woman', 'NN'), (u'who', 'WP'), (u'called', 'VBD'), (u'America', 'NNP'), (u'to', 'TO'), (u'its', 'PRP$'), (u'founding', 'NN'), (u'ideals', 'NNS'), (u'and', 'CC'), (u'carried', 'VBD'), (u'on', 'IN'), (u'a', 'DT'), (u'noble', 'JJ'), (u'dream', 'NN'), (u'.', '.')]\n",
      "[(u'Tonight', 'NN'), (u'we', 'PRP'), (u'are', 'VBP'), (u'comforted', 'VBN'), (u'by', 'IN'), (u'the', 'DT'), (u'hope', 'NN'), (u'of', 'IN'), (u'a', 'DT'), (u'glad', 'JJ'), (u'reunion', 'NN'), (u'with', 'IN'), (u'the', 'DT'), (u'husband', 'NN'), (u'who', 'WP'), (u'was', 'VBD'), (u'taken', 'VBN'), (u'so', 'RB'), (u'long', 'RB'), (u'ago', 'RB'), (u',', ','), (u'and', 'CC'), (u'we', 'PRP'), (u'are', 'VBP'), (u'grateful', 'JJ'), (u'for', 'IN'), (u'the', 'DT'), (u'good', 'JJ'), (u'life', 'NN'), (u'of', 'IN'), (u'Coretta', 'NNP'), (u'Scott', 'NNP'), (u'King', 'NNP'), (u'.', '.')]\n",
      "[(u'(', '('), (u'Applause', 'NNP'), (u'.', '.'), (u')', ')')]\n",
      "[(u'President', 'NNP'), (u'George', 'NNP'), (u'W.', 'NNP'), (u'Bush', 'NNP'), (u'reacts', 'VBZ'), (u'to', 'TO'), (u'applause', 'VB'), (u'during', 'IN'), (u'his', 'PRP$'), (u'State', 'NNP'), (u'of', 'IN'), (u'the', 'DT'), (u'Union', 'NNP'), (u'Address', 'NNP'), (u'at', 'IN'), (u'the', 'DT'), (u'Capitol', 'NNP'), (u',', ','), (u'Tuesday', 'NNP'), (u',', ','), (u'Jan', 'NNP'), (u'.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "- chunking is creating a set of conditions on which chunks all of the data the conforms with the conditions then outputs them into a list.\n",
    "\n",
    "\n",
    "# Chinking\n",
    "\n",
    "- chinking also works with chunking, it is the reverse process of chunking, it simply means we are going to remove something.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free contents from nltk, the following are from the state of the union address of George W. Bush\n",
    "train_set = state_union.raw('2005-GWBush.txt')\n",
    "test_set = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the training set, splitting them according to the punktsentencetokenizer algorithm\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing and testing with the test set\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%nltk` not found.\n"
     ]
    }
   ],
   "source": [
    "%nltk inline\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            #This means we're removing from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "            chunGramwithChink = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "- named entity recognition sets a specific rules on which if this tokenized and POS tagged word adheres to this it is a named entity\n",
    "\n",
    "- sample: Name George W. Bush\n",
    "\n",
    "NE Type and Examples\n",
    "\n",
    "- ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "- PERSON       - Eddy Bonte, President Obama\n",
    "- LOCATION     - Murray River, Mount Everest\n",
    "- DATE         - June, 2008-06-29\n",
    "- TIME         - two fifty a m, 1:30 p.m.\n",
    "- MONEY        - 175 million Canadian Dollars, GBP 10.40\n",
    "- PERCENT      - twenty pct, 18.75 %\n",
    "- FACILITY     - Washington Monument, Stonehenge\n",
    "- GPE          - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free contents from nltk, the following are from the state of the union address of George W. Bush\n",
    "train_set = state_union.raw('2005-GWBush.txt')\n",
    "test_set = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the training set, splitting them according to the punktsentencetokenizer algorithm\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing and testing with the test set\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%nltk` not found.\n"
     ]
    }
   ],
   "source": [
    "%nltk\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #Create a variable holding the named entity chunked\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
    "            namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "\n",
    "- lemmatizing is knowing the simplest word that is associated with the word being tested without losing information and making it more useful.\n",
    "- We could also use lemmatizing as a means of chunking or grouping a specific set and making it more compact not like stemming\n",
    "- lemmatizing can be configured using the part of speech keyword. sample pos='a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('better'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('better', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora\n",
    "\n",
    "- again, we will be discussing about corpora, it is a collection of data sets that can be used from the natural language tool kit downloadable library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample import of a corpora downloaded from the nltk.download()\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet\n",
    "\n",
    "- is a way on which we can access the different possibilities of information given by a word, information like antonyms, synonyms, definition / description, examples and a lot more\n",
    "\n",
    "# Synsets\n",
    "\n",
    "- enables us to get the different information of the word which we passed through the function\n",
    "- a set of synonyms from a single information source or a single meaning\n",
    "\n",
    "# Lemmas\n",
    "\n",
    "- enables us to get the synonymous words of the word that we passed in, not only the synonymous word of one meaning but also the synoynmous word of a different meaning, after passing synsets.\n",
    "\n",
    "- sample: Good may have a synonymous word of good\n",
    "\n",
    "# Similarity\n",
    "\n",
    "- similarity gives us a percentage on which 2 words are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing wordnet\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a variable that will hold the synsets of the word program\n",
    "#We can change the program inside the module synsets\n",
    "syns = wordnet.synsets(\"able\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('able.a.01')\n"
     ]
    }
   ],
   "source": [
    "#prints out the complete synset value of the first object on the synset list\n",
    "print(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able.a.01\n"
     ]
    }
   ],
   "source": [
    "#prints out the name first synsets of the word program\n",
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('able.a.01.able')]\n"
     ]
    }
   ],
   "source": [
    "#prints out the lemmas of the synset of the first value from the synset list\n",
    "print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able\n"
     ]
    }
   ],
   "source": [
    "#prints out the name of the first lemmas of the synset of the first value form the synset list\n",
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('unable.a.01.unable')]\n",
      "unable\n"
     ]
    }
   ],
   "source": [
    "#prints out the antonym with its properties of the word being passed through\n",
    "print(syns[0].lemmas()[0].antonyms())\n",
    "\n",
    "#prints out the antonym of the word being passed through\n",
    "print(syns[0].lemmas()[0].antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arranging everything into synonyms, antonyms and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.380952380952\n"
     ]
    }
   ],
   "source": [
    "#wup_similarity will rate the word being compared on how related they are using wu and p's taxonomy algorithm\n",
    "word1 = wordnet.synsets('ship')\n",
    "word2 = wordnet.synsets('cactus')\n",
    "\n",
    "print(word1[0].wup_similarity(word2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857142857143\n"
     ]
    }
   ],
   "source": [
    "#wup_similarity will rate the word being compared on how related they are using wu and p's taxonomy algorithm\n",
    "word1 = wordnet.synsets('cat')\n",
    "word2 = wordnet.synsets('dog')\n",
    "\n",
    "print(word1[0].wup_similarity(word2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (Bag of Words)\n",
    "\n",
    "Process:\n",
    "1. tokenizing the words\n",
    "2. classifying using frequency distribution\n",
    "3. testing\n",
    "4. removing stop words\n",
    "5. testing if how many positive and negative words are in a sentence\n",
    "\n",
    "\n",
    "\n",
    "- text classification is the process of classifying the text or corpus into positives or negatives, the user can say what will be the outcome of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ternary \n",
    "# aa = [1,2,3]\n",
    "# bb = [0,0,0]\n",
    "\n",
    "# #foo = [(i.j) for i in aa for j in bb if i==1]\n",
    "# print foo\n",
    "\n",
    "# for i in aa:\n",
    "#     for j in bb:\n",
    "#         if i == 1:\n",
    "#             print (i,j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category) \n",
    "             for category in movie_reviews.categories() \n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print(all_words[\"stupid\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
