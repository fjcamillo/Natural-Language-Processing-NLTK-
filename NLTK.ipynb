{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Notebook\n",
    "\n",
    "Tokenizer\n",
    "     - Word Tokenizer\n",
    "     - Sentence Tokenizer\n",
    "\n",
    "Corpora\n",
    "    - body of text, e.g. english language\n",
    "lexicon\n",
    "    - word and their meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Dear Mr. Francisc, Today we will be going to study about NLTK, a natural language processing library. Built for python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'Mr.', 'Francisc', ',', 'Today', 'we', 'will', 'be', 'going', 'to', 'study', 'about', 'NLTK', ',', 'a', 'natural', 'language', 'processing', 'library', '.', 'Built', 'for', 'python']\n"
     ]
    }
   ],
   "source": [
    "#prints out the tokenized text by word\n",
    "textReturn = word_tokenize(text)\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear Mr. Francisc, Today we will be going to study about NLTK, a natural language processing library.', 'Built for python']\n"
     ]
    }
   ],
   "source": [
    "words = sent_tokenize(text)\n",
    "\n",
    "#prints out the tokenized text by sentence\n",
    "print(sent_tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    " - stop words are words that will not give out information and may be neglected and not added to the feature set of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_words = [w for w in textReturn if not w in stop_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'Mr.', 'Francisc', ',', 'Today', 'going', 'study', 'NLTK', ',', 'natural', 'language', 'processing', 'library', '.', 'Built', 'python']\n"
     ]
    }
   ],
   "source": [
    "#printing filtered words\n",
    "print filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "- stemming is finding the word closest to the meaning of the actual word being used in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "pythonista\n",
      "pythonli\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "new_word = \"Good Morning, we are hear to learn about stemming, today we must know what is stemming and how it works\"\n",
    "related = ['python', 'pythoner', 'pythonista', 'pythonly', 'pythoning']\n",
    "\n",
    "for pyn in related:\n",
    "    print(ps.stem(pyn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good\n",
      "Morn\n",
      ",\n",
      "we\n",
      "are\n",
      "hear\n",
      "to\n",
      "learn\n",
      "about\n",
      "stem\n",
      ",\n",
      "today\n",
      "we\n",
      "must\n",
      "know\n",
      "what\n",
      "is\n",
      "stem\n",
      "and\n",
      "how\n",
      "it\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_word)\n",
    "for pyn in words:\n",
    "    print(ps.stem(pyn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "- part of speech tagging is tagging the different words in a sentence or paragraph on what they are.\n",
    "\n",
    "POS tag list:\n",
    "\n",
    "- CC    ==> coordinating conjunction\n",
    "- CD\t==> cardinal digit\n",
    "- DT\t==> determiner\n",
    "- EX    ==> existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW\t==> foreign word\n",
    "- IN\t==> preposition/subordinating conjunction\n",
    "- JJ\t==> adjective\t'big'\n",
    "- JJR\t==> adjective, comparative\t'bigger'\n",
    "- JJS\t==> adjective, superlative\t'biggest'\n",
    "- LS\t==> list marker\t1)\n",
    "- MD\t==> modal\tcould, will\n",
    "- NN\t==> noun, singular 'desk'\n",
    "- NNS\t==> noun plural\t'desks'\n",
    "- NNP\t==> proper noun, singular\t'Harrison'\n",
    "- NNPS\t==> proper noun, plural\t'Americans'\n",
    "- PDT\t==> predeterminer\t'all the kids'\n",
    "- POS\t==> possessive ending\tparent's\n",
    "- PRP\t==> personal pronoun\tI, he, she\n",
    "- PRP(DollarSign)\t==> possessive pronoun\tmy, his, hers\n",
    "- RB\t==> adverb\tvery, silently,\n",
    "- RBR\t==> adverb, comparative\tbetter\n",
    "- RBS\t==> adverb, superlative\tbest\n",
    "- RP\t==> particle\tgive up\n",
    "- TO\t==> to\tgo 'to' the store.\n",
    "- UH\t==> interjection\terrrrrrrrm\n",
    "- VB\t==> verb, base form\ttake\n",
    "- VBD\t==> verb, past tense\ttook\n",
    "- VBG\t==> verb, gerund/present participle\ttaking\n",
    "- VBN\t==> verb, past participle\ttaken\n",
    "- VBP\t==> verb, sing. present, non-3d\ttake\n",
    "- VBZ\t==> verb, 3rd person sing. present\ttakes\n",
    "- WDT\t==> wh-determiner\twhich\n",
    "- WP\t==> wh-pronoun\twho, what\n",
    "- WP(DollarSign)\t==> possessive wh-pronoun\twhose\n",
    "- WRB\t==> wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free contents from nltk, the following are from the state of the union address of George W. Bush\n",
    "train_set = state_union.raw('2005-GWBush.txt')\n",
    "test_set = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the training set, splitting them according to the punktsentencetokenizer algorithm\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing and testing with the test set\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'PRESIDENT', 'NNP'), (u'GEORGE', 'NNP'), (u'W.', 'NNP'), (u'BUSH', 'NNP'), (u\"'S\", 'POS'), (u'ADDRESS', 'NNP'), (u'BEFORE', 'IN'), (u'A', 'NNP'), (u'JOINT', 'NNP'), (u'SESSION', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'CONGRESS', 'NNP'), (u'ON', 'NNP'), (u'THE', 'NNP'), (u'STATE', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'UNION', 'NNP'), (u'January', 'NNP'), (u'31', 'CD'), (u',', ','), (u'2006', 'CD'), (u'THE', 'NNP'), (u'PRESIDENT', 'NNP'), (u':', ':'), (u'Thank', 'NNP'), (u'you', 'PRP'), (u'all', 'DT'), (u'.', '.')]\n",
      "[(u'Mr.', 'NNP'), (u'Speaker', 'NNP'), (u',', ','), (u'Vice', 'NNP'), (u'President', 'NNP'), (u'Cheney', 'NNP'), (u',', ','), (u'members', 'NNS'), (u'of', 'IN'), (u'Congress', 'NNP'), (u',', ','), (u'members', 'NNS'), (u'of', 'IN'), (u'the', 'DT'), (u'Supreme', 'NNP'), (u'Court', 'NNP'), (u'and', 'CC'), (u'diplomatic', 'JJ'), (u'corps', 'NN'), (u',', ','), (u'distinguished', 'JJ'), (u'guests', 'NNS'), (u',', ','), (u'and', 'CC'), (u'fellow', 'JJ'), (u'citizens', 'NNS'), (u':', ':'), (u'Today', 'VB'), (u'our', 'PRP$'), (u'nation', 'NN'), (u'lost', 'VBD'), (u'a', 'DT'), (u'beloved', 'VBN'), (u',', ','), (u'graceful', 'JJ'), (u',', ','), (u'courageous', 'JJ'), (u'woman', 'NN'), (u'who', 'WP'), (u'called', 'VBD'), (u'America', 'NNP'), (u'to', 'TO'), (u'its', 'PRP$'), (u'founding', 'NN'), (u'ideals', 'NNS'), (u'and', 'CC'), (u'carried', 'VBD'), (u'on', 'IN'), (u'a', 'DT'), (u'noble', 'JJ'), (u'dream', 'NN'), (u'.', '.')]\n",
      "[(u'Tonight', 'NN'), (u'we', 'PRP'), (u'are', 'VBP'), (u'comforted', 'VBN'), (u'by', 'IN'), (u'the', 'DT'), (u'hope', 'NN'), (u'of', 'IN'), (u'a', 'DT'), (u'glad', 'JJ'), (u'reunion', 'NN'), (u'with', 'IN'), (u'the', 'DT'), (u'husband', 'NN'), (u'who', 'WP'), (u'was', 'VBD'), (u'taken', 'VBN'), (u'so', 'RB'), (u'long', 'RB'), (u'ago', 'RB'), (u',', ','), (u'and', 'CC'), (u'we', 'PRP'), (u'are', 'VBP'), (u'grateful', 'JJ'), (u'for', 'IN'), (u'the', 'DT'), (u'good', 'JJ'), (u'life', 'NN'), (u'of', 'IN'), (u'Coretta', 'NNP'), (u'Scott', 'NNP'), (u'King', 'NNP'), (u'.', '.')]\n",
      "[(u'(', '('), (u'Applause', 'NNP'), (u'.', '.'), (u')', ')')]\n",
      "[(u'President', 'NNP'), (u'George', 'NNP'), (u'W.', 'NNP'), (u'Bush', 'NNP'), (u'reacts', 'VBZ'), (u'to', 'TO'), (u'applause', 'VB'), (u'during', 'IN'), (u'his', 'PRP$'), (u'State', 'NNP'), (u'of', 'IN'), (u'the', 'DT'), (u'Union', 'NNP'), (u'Address', 'NNP'), (u'at', 'IN'), (u'the', 'DT'), (u'Capitol', 'NNP'), (u',', ','), (u'Tuesday', 'NNP'), (u',', ','), (u'Jan', 'NNP'), (u'.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "- chunking is creating a set of conditions on which chunks all of the data the conforms with the conditions then outputs them into a list.\n",
    "\n",
    "\n",
    "# Chinking\n",
    "\n",
    "- chinking also works with chunking, it is the reverse process of chunking, it simply means we are going to remove something.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free contents from nltk, the following are from the state of the union address of George W. Bush\n",
    "train_set = state_union.raw('2005-GWBush.txt')\n",
    "test_set = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the training set, splitting them according to the punktsentencetokenizer algorithm\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing and testing with the test set\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%nltk` not found.\n"
     ]
    }
   ],
   "source": [
    "%nltk inline\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            #This means we're removing from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "            chunGramwithChink = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "- named entity recognition sets a specific rules on which if this tokenized and POS tagged word adheres to this it is a named entity\n",
    "\n",
    "- sample: Name George W. Bush\n",
    "\n",
    "NE Type and Examples\n",
    "\n",
    "- ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "- PERSON       - Eddy Bonte, President Obama\n",
    "- LOCATION     - Murray River, Mount Everest\n",
    "- DATE         - June, 2008-06-29\n",
    "- TIME         - two fifty a m, 1:30 p.m.\n",
    "- MONEY        - 175 million Canadian Dollars, GBP 10.40\n",
    "- PERCENT      - twenty pct, 18.75 %\n",
    "- FACILITY     - Washington Monument, Stonehenge\n",
    "- GPE          - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free contents from nltk, the following are from the state of the union address of George W. Bush\n",
    "train_set = state_union.raw('2005-GWBush.txt')\n",
    "test_set = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the training set, splitting them according to the punktsentencetokenizer algorithm\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing and testing with the test set\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%nltk` not found.\n"
     ]
    }
   ],
   "source": [
    "%nltk\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #Create a variable holding the named entity chunked\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
    "            namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "\n",
    "- lemmatizing is knowing the simplest word that is associated with the word being tested without losing information and making it more useful.\n",
    "- We could also use lemmatizing as a means of chunking or grouping a specific set and making it more compact not like stemming\n",
    "- lemmatizing can be configured using the part of speech keyword. sample pos='a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('better'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('better', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora\n",
    "\n",
    "- again, we will be discussing about corpora, it is a collection of data sets that can be used from the natural language tool kit downloadable library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample import of a corpora downloaded from the nltk.download()\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet\n",
    "\n",
    "- is a way on which we can access the different possibilities of information given by a word, information like antonyms, synonyms, definition / description, examples and a lot more\n",
    "\n",
    "# Synsets\n",
    "\n",
    "- enables us to get the different information of the word which we passed through the function\n",
    "- a set of synonyms from a single information source or a single meaning\n",
    "\n",
    "# Lemmas\n",
    "\n",
    "- enables us to get the synonymous words of the word that we passed in, not only the synonymous word of one meaning but also the synoynmous word of a different meaning, after passing synsets.\n",
    "\n",
    "- sample: Good may have a synonymous word of good\n",
    "\n",
    "# Similarity\n",
    "\n",
    "- similarity gives us a percentage on which 2 words are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing wordnet\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a variable that will hold the synsets of the word program\n",
    "#We can change the program inside the module synsets\n",
    "syns = wordnet.synsets(\"able\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('able.a.01')\n"
     ]
    }
   ],
   "source": [
    "#prints out the complete synset value of the first object on the synset list\n",
    "print(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able.a.01\n"
     ]
    }
   ],
   "source": [
    "#prints out the name first synsets of the word program\n",
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('able.a.01.able')]\n"
     ]
    }
   ],
   "source": [
    "#prints out the lemmas of the synset of the first value from the synset list\n",
    "print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able\n"
     ]
    }
   ],
   "source": [
    "#prints out the name of the first lemmas of the synset of the first value form the synset list\n",
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('unable.a.01.unable')]\n",
      "unable\n"
     ]
    }
   ],
   "source": [
    "#prints out the antonym with its properties of the word being passed through\n",
    "print(syns[0].lemmas()[0].antonyms())\n",
    "\n",
    "#prints out the antonym of the word being passed through\n",
    "print(syns[0].lemmas()[0].antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arranging everything into synonyms, antonyms and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.380952380952\n"
     ]
    }
   ],
   "source": [
    "#wup_similarity will rate the word being compared on how related they are using wu and p's taxonomy algorithm\n",
    "word1 = wordnet.synsets('ship')\n",
    "word2 = wordnet.synsets('cactus')\n",
    "\n",
    "print(word1[0].wup_similarity(word2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857142857143\n"
     ]
    }
   ],
   "source": [
    "#wup_similarity will rate the word being compared on how related they are using wu and p's taxonomy algorithm\n",
    "word1 = wordnet.synsets('cat')\n",
    "word2 = wordnet.synsets('dog')\n",
    "\n",
    "print(word1[0].wup_similarity(word2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (Bag of Words)\n",
    "\n",
    "Process:\n",
    "1. tokenizing the words\n",
    "2. classifying using frequency distribution\n",
    "3. testing\n",
    "4. removing stop words\n",
    "5. testing if how many positive and negative words are in a sentence\n",
    "\n",
    "\n",
    "\n",
    "- text classification is the process of classifying the text or corpus into positives or negatives, the user can say what will be the outcome of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ternary \n",
    "# aa = [1,2,3]\n",
    "# bb = [0,0,0]\n",
    "\n",
    "# #foo = [(i.j) for i in aa for j in bb if i==1]\n",
    "# print foo\n",
    "\n",
    "# for i in aa:\n",
    "#     for j in bb:\n",
    "#         if i == 1:\n",
    "#             print (i,j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates a documents variable holding a list from words within the categories of the movie_review module\n",
    "documents = [(list(movie_reviews.words(fileid)), category) \n",
    "             for category in movie_reviews.categories() \n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomizes the positioning of the items inside the list\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lower cases all the items inside the list.\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates a frequency distribution of all the items\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "#prints the most common words according to frequency\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "#prints the number of times the word stupid is mentioned\n",
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting words to Features\n",
    "\n",
    "- converting words to features means that we must select the words that gives us information then remove all the words that doesn't give any information, we could also neglect words if they are not part of a given sample size on which we would train our Machine Learning Algorithm, the sample size is dependent to the user/analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a documents variable holding a list from words within the categories of the movie_review module\n",
    "documents = [(list(movie_reviews.words(fileid)), category) \n",
    "             for category in movie_reviews.categories() \n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomizes the positioning of the items inside the list\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lower cases all the items inside the list.\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a frequency distribution of all the items\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier with NLTK\n",
    "\n",
    "- nltk has a built in naive bayes classifier like the one from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training sets will act as the main information on which the machine will learn using labels and features\n",
    "training_set = featuresets[:1900]\n",
    "\n",
    "#Testing sets will act as the checker of our machine learning algorithm it. The machine will test\n",
    "#the learned algorithm on the testing set to know the accuracy of the learned algorithm\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We created a Naive Bayes Classifier algorithm that will train on the training_set variable\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Classifier Accuracy Percentage: ', 67.0)\n"
     ]
    }
   ],
   "source": [
    "#We the print out the\n",
    "print(\"Classifier Accuracy Percentage: \", (nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               insulting = True              neg : pos    =      9.8 : 1.0\n",
      "                    sans = True              neg : pos    =      8.4 : 1.0\n",
      "            refreshingly = True              pos : neg    =      8.3 : 1.0\n",
      "              mediocrity = True              neg : pos    =      7.7 : 1.0\n",
      "                    hugo = True              pos : neg    =      7.6 : 1.0\n",
      "                  wasted = True              neg : pos    =      6.7 : 1.0\n",
      "             bruckheimer = True              neg : pos    =      6.4 : 1.0\n",
      "             overwhelmed = True              pos : neg    =      6.3 : 1.0\n",
      "               uplifting = True              pos : neg    =      5.8 : 1.0\n",
      "                   wires = True              neg : pos    =      5.7 : 1.0\n",
      "               dismissed = True              pos : neg    =      5.6 : 1.0\n",
      "                    lang = True              pos : neg    =      5.6 : 1.0\n",
      "                 topping = True              pos : neg    =      5.6 : 1.0\n",
      "                     ugh = True              neg : pos    =      5.4 : 1.0\n",
      "                  stinks = True              neg : pos    =      5.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Classifiers in NLTK (using Pickle)\n",
    "\n",
    "- saving classifiers could save us huge amount of time in traing and retraining our machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickle a standard library from python\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open a naivebeyes.pickle file on the path your on, 'wb' means write in bytes - Python 3.x.x\n",
    "save_classifier = open('naivebeyes.pickle', 'wb')\n",
    "\n",
    "#create a dump, pass in the classifier we created before and the file to save into\n",
    "pickle.dump(classifier, save_classifier)\n",
    "\n",
    "#close the file \n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opening file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open up the file\n",
    "classifier_f = open('naivebeyes.pickle', 'rb')\n",
    "\n",
    "#load the pickle file\n",
    "classifier_f = pickle.load(classifier_f)\n",
    "\n",
    "#close the file\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.svm import NuSVC,SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
